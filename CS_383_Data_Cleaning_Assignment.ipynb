{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sCuXLJwsodho"
      },
      "outputs": [],
      "source": [
        "# Imports and pip installations (if needed)\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKX0OrMtqAKI"
      },
      "source": [
        "# Part 1: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR38S5mRo0r5"
      },
      "outputs": [],
      "source": [
        "# Load the given datasets\n",
        "categorical_df = pd.read_csv('chronic_kidney_disease_categorical.csv')\n",
        "numerical_df = pd.read_csv('chronic_kidney_disease_numerical.csv')\n",
        "\n",
        "# Print the data\n",
        "categorical_df.head(), numerical_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93V45BfjqI6i"
      },
      "source": [
        "# Part 2: Analyze the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08JU4hVnqMLF"
      },
      "source": [
        "Refer to this: https://archive.ics.uci.edu/dataset/336/chronic+kidney+disease\n",
        "\n",
        "Explain what the each data is in your own words. What are the features and labels? Are the features in the given datasets : categorical, numerical or both? Give 3 examples of categorical and numerical columns each (if they exist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRSbXNfT09Wu"
      },
      "source": [
        "Answer: \n",
        "\n",
        "The features are the various attributes that are used to predict whether the patient has chronic kidney disease, these include blood pressure, specific gravity etc. The dataset also mentions the type of data, whether it's an integer, categorical, binary and its units. These are used as the variables and parameters used when considering if someone has the disease or not. \n",
        "\n",
        "The label is either ckd, notckd, which represents the target variable: whether the patient has the chronic kidney disease or not.\n",
        "\n",
        "Age, Blood Pressure, Blood Glucose Random, Blood Urea Serum Creatinine, Sodium, Potassium, Hemoglobin, Packed cell volume, White blood cell count, red blood cell count – These are all the numerical columns of the dataset.\n",
        "\n",
        "Albumin, sugar are both categorical and numerical.\n",
        "\n",
        "Red blood cell, pus cell, pus cell clumps, bacteria, hypertension, diabetes mellitus, coronary artery disease, appetite, pedal edema, anemia – Thse are all the categorical columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6raapHMwfID"
      },
      "source": [
        "# Part 3: Data Preprocessing\n",
        "\n",
        "A fundamental skill in Machine Learning is mastering the art of data cleaning and preprocessing. In this assignment, you will learn and apply essential data cleaning techniques to transform a raw dataset into a clean, ready-to-use form which you can use for regression or classification tasks. By the end of this assignment, you'll have a fully clean dataset and a solid foundation in preparing data for various machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FYmj6zTJk-"
      },
      "source": [
        "## Part 3.1 : Drop Duplicate rows\n",
        "\n",
        "Let's start by checking if the given datasets have any duplicate rows (same Unique Id). Use pandas to identify and remove these duplicate rows from the given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjkwmspsQeXP"
      },
      "outputs": [],
      "source": [
        "# For the numerical dataset, check if there are duplicate rows in the dataset. If yes, print total number of duplicate rows\n",
        "num_duplicates = numerical_df.duplicated(subset='unique_id').sum()\n",
        "print(num_duplicates)\n",
        "\n",
        "# Drop these duplicate rows\n",
        "num_df_noduplicates = numerical_df.drop_duplicates(subset='unique_id')\n",
        "\n",
        "# Repeat the same for categorical dataset. Print the duplicate rows and drop them\n",
        "cat_duplicates = categorical_df.duplicated(subset='unique_id').sum()\n",
        "print(cat_duplicates)\n",
        "\n",
        "cat_df_noduplicates = categorical_df.drop_duplicates(subset='unique_id')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBFadPsfBMYQ"
      },
      "source": [
        "## Part 3.2: Combine two differents datasets\n",
        "\n",
        "A good skill to have is to know how to combine 2 different datasets.\n",
        "\n",
        "Are all the unique ids are present in both datasets? Why do you think so? If not, what do the rows that are missing from one of the datasets look like in the combined table?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KFoNLYtVYYS"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYDcaPFQBtbm"
      },
      "outputs": [],
      "source": [
        "# Merge the two given numerical and categorical datasets based on their unique_ID.\n",
        "combined_df = pd.merge(num_df_noduplicates, cat_df_noduplicates,  on='unique_id', how='outer')\n",
        "\n",
        "#Print the combined dataset\n",
        "combined_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgKzV1mFwksD"
      },
      "source": [
        "## Part 3.3: Rows with Missing values\n",
        "\n",
        "Removing missing values from a dataset is important for classification because it ensures the model is trained on complete and accurate data, leading to better performance and reliable predictions. Incomplete data can introduce bias and errors, negatively impacting the model's effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "octo6qxgxstJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of rows that contain atleast one missing value\n",
        "rows_atleast1missing = combined_df.isnull().any(axis=1).sum()\n",
        "total_rows = combined_df.shape[0]\n",
        "percentage_missing = (rows_atleast1missing / total_rows) * 100\n",
        "\n",
        "# Print %\n",
        "percentage_missing = float(percentage_missing)\n",
        "percentage_missing\n",
        "\n",
        "# Drop these rows from the dataset\n",
        "combined_df_noNaN = combined_df.dropna()\n",
        "\n",
        "# Print the Dataset\n",
        "combined_df_noNaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHLmE7i75-Xu"
      },
      "source": [
        "## Part 3.4: Sort the dataset according to the Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzvGw6Xt-M-k"
      },
      "outputs": [],
      "source": [
        "# Sort the dataset according to the values in 'Target' column. Make sure reset the indices after sorting\n",
        "sorted_target_df = combined_df_noNaN.sort_values(by='Target')\n",
        "sorted_reset_df = sorted_target_df.reset_index(drop=True)\n",
        "\n",
        "# Print the dataset\n",
        "sorted_reset_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZWpxx0pzoRc"
      },
      "source": [
        "## Part 3.5: Encoding Categorical data\n",
        "\n",
        "In this step, we identify and process the categorical columns in the sorted dataset. We map each unique value in these columns to separate \"dummy\" columns.\n",
        "\n",
        "For example, the column 'rbc' will be transformed into two columns 'rbc_normal' and 'rbc_abnormal'. If a row's value in 'rbc' is 'normal', the 'rbc_normal' column will be set to 1 and 'rbc_abnormal' will be set to 0.\n",
        "\n",
        "\n",
        "**Note: Find a correct pandas function to do this **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9R8ldVIwoc-"
      },
      "outputs": [],
      "source": [
        "# Write code here\n",
        "categorical_columns = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'Target']\n",
        "\n",
        "encoded_df = pd.get_dummies(sorted_target_df, columns=categorical_columns).astype(int)\n",
        "\n",
        "# Print the dataset\n",
        "encoded_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fvO_TZ0Uvgc"
      },
      "source": [
        "In the example we went through above, another solution is to have a single column for the binary variable. In the downstream modeling would this be equivalent? What effect would this have if the categorical variable could take more than 2 values? For example, let's say we have a categorical feature that is \"type of condiment\" that can take 5 separate values and we are trying to predict the rating of a particular sandwich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocVyS5yA-Fv"
      },
      "source": [
        "## Part 3.6 : Remove Outliers from Numerical Columns\n",
        "\n",
        "Outliers can disproportionately influence the fit of a regression model, potentially leading to a model that does not generalize well therefore it is important that we remove outliers from the numerical columns of the dataset.\n",
        "\n",
        "For this dataset, we define an outlier to be 3 times the standard deviation from the mean. Drop these outliers from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdupS0h3h6Cl"
      },
      "outputs": [],
      "source": [
        "# Remove outliers\n",
        "numerical_columns = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wbcc', 'rbcc']\n",
        "\n",
        "means = {}\n",
        "std_devs = {}\n",
        "\n",
        "for column in numerical_columns:\n",
        "    means[column] = encoded_df[column].mean()\n",
        "    std_devs[column] = encoded_df[column].std()\n",
        "\n",
        "no_outliers_df = encoded_df.copy()\n",
        "for column in numerical_columns:\n",
        "    lower_bound = means[column] - 3 * std_devs[column]\n",
        "    upper_bound = means[column] + 3 * std_devs[column]\n",
        "    no_outliers_df = no_outliers_df[(no_outliers_df[column] >= lower_bound) & (no_outliers_df[column] <= upper_bound)]\n",
        "\n",
        "# Print the dataset\n",
        "no_outliers_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SSSwLfFjpL4"
      },
      "source": [
        "## Part 3.7 : Normalize the Numerical Columns\n",
        "\n",
        "Normalizing numerical attributes ensures that all features contribute equally to the model by scaling them to a consistent range, which improves model performance and convergence. It prevents features with larger scales from disproportionately influencing the model's learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4xGDDewA859"
      },
      "outputs": [],
      "source": [
        "scale = MinMaxScaler()\n",
        "\n",
        "normalized_df = no_outliers_df.copy()\n",
        "\n",
        "normalized_df[numerical_columns] = scale.fit_transform(normalized_df[numerical_columns])\n",
        "\n",
        "# Print the dataset\n",
        "normalized_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fomr0fL847aq"
      },
      "source": [
        "## Part 3.8: Remove Unnecessary columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rapn8p607X1e"
      },
      "source": [
        "Are there any columns in this dataset which are not appropriate for modeling and predictions? Which column(s)? Justify their exclusion and remove them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ1r6z118C9Z"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QIC2yyk5QO4"
      },
      "outputs": [],
      "source": [
        "#Remove that column\n",
        "normalized_sorted_df = normalized_df.sort_values(by='unique_id').reset_index(drop=True)\n",
        "\n",
        "new_normalized_sorted_df = normalized_sorted_df.drop(columns=['unique_id'])\n",
        "\n",
        "# Print the dataset\n",
        "new_normalized_sorted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3.9: Export the Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you've completed these cleaning steps you should have a pandas dataframe which is much cleaner and ready for modeling. Our final step is to save our work. Export the DataFrame to a two new formats: csv and json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the dataframe to a new csv file\n",
        "new_normalized_sorted_df.to_csv('cleaned_kidney_disease_data.csv', index=False)\n",
        "\n",
        "# Save as JSON\n",
        "new_normalized_sorted_df.to_json('cleaned_kidney_disease_data.json', orient='records')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Data conversions with Large Language Models\n",
        "\n",
        "One powerful use case of ChatGPT (and other generative language models) is cleaning and transforming data. In some cases, these models can directly manipulate loosely structured data that you provide to them into a standard format. In the other cases, you can often prompt the model to create a conversion or extraction script for you in python or Pandas and then run it on your own. \n",
        "\n",
        "In this part of the assignment you will prompt 383GPT to explore these capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4.1 GPT Data Manipulation\n",
        "\n",
        "Take the cleaned dataset that you created in part three and output the top 15 rows of that dataset. Then copy the terminal output, open 383gpt and ask it to convert that output to a markdown table. Paste that markdown table in the cell bellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_15_rows = new_normalized_sorted_df.head(15)\n",
        "top_15_rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paste the markdown table here\n",
        "\n",
        "|    |       age |    bp |       bgr |       bu |     sc |      sod |    pot |      hemo |      pcv |      wbcc | ... |   cad_no |   cad_yes |   appet_good |   appet_poor |   pe_no |   pe_yes |   ane_no |   ane_yes |   Target_ckd |   Target_notckd |\n",
        "|----|-----------|-------|-----------|----------|--------|----------|--------|------------|----------|-----------|-----|----------|-----------|---------------|---------------|---------|----------|----------|-----------|---------------|-----------------|\n",
        "|  0 | 0.783784  |  0.00 | 0.154506  | 0.111111 | 0.000  | 1.000000 | 0.25   | 0.750000   | 0.684211 | 0.306452  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  1 | 0.472973  |  0.50 | 0.223176  | 0.098039 | 0.000  | 0.600000 | 0.75   | 1.000000   | 0.657895 | 0.387097  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  2 | 0.486486  |  0.25 | 0.098712  | 0.143791 | 0.000  | 0.766667 | 0.50   | 0.916667   | 0.710526 | 0.225806  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  3 | 0.918919  |  0.00 | 0.077253  | 0.261438 | 0.000  | 0.900000 | 0.25   | 1.000000   | 0.973684 | 0.137097  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  4 | 0.851351  |  0.25 | 0.832618  | 0.503268 | 0.250  | 0.333333 | 0.50   | 0.583333   | 0.552632 | 0.427419  | ... |        0 |         1 |             1 |             0 |       0 |        1 |        1 |         0 |             1 |                 0 |\n",
        "|  5 | 0.418919  |  0.00 | 0.167382  | 0.241830 | 0.125  | 0.700000 | 0.50   | 0.833333   | 0.842105 | 0.217742  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  6 | 0.567568  |  0.50 | 0.270386  | 0.843137 | 1.000  | 0.400000 | 0.75   | 0.416667   | 0.526316 | 0.153226  | ... |        1 |         0 |             1 |             0 |       0 |        1 |        1 |         0 |             1 |                 0 |\n",
        "|  7 | 0.621622  |  0.50 | 0.236052  | 0.078431 | 0.125  | 0.633333 | 0.50   | 0.916667   | 0.710526 | 0.032258  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  8 | 0.310811  |  0.50 | 0.055794  | 0.254902 | 0.000  | 0.633333 | 0.25   | 1.000000   | 0.631579 | 0.451613  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "|  9 | 0.905405  |  1.00 | 0.965665  | 0.522876 | 0.625  | 0.666667 | 0.00   | 0.333333   | 0.368421 | 0.217742  | ... |        0 |         1 |             0 |             1 |       1 |        0 |        1 |         0 |             1 |                 0 |\n",
        "| 10 | 0.837838  |  0.00 | 0.236052  | 0.202614 | 0.125  | 0.633333 | 0.25   | 1.000000   | 0.894737 | 0.193548  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "| 11 | 0.716216  |  0.00 | 0.184549  | 0.084967 | 0.125  | 0.633333 | 0.25   | 0.833333   | 1.000000 | 0.177419  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "| 12 | 0.567568  |  0.00 | 0.180258  | 0.222222 | 0.125  | 0.733333 | 0.50   | 0.750000   | 0.736842 | 0.411290  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "| 13 | 0.810811  |  0.25 | 0.158798  | 0.039216 | 0.125  | 0.666667 | 0.25   | 0.666667   | 0.684211 | 0.540323  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n",
        "| 14 | 0.608108  |  0.00 | 0.124464  | 0.183007 | 0.000  | 0.500000 | 0.25   | 0.666667   | 0.868421 | 0.322581  | ... |        1 |         0 |             1 |             0 |       1 |        0 |        1 |         0 |             0 |                 1 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Caution: ** while language models can perform data conversions they also can * hallucinate * during this process, particularly for bigger datasets. Reflect on this below, how could you mitigate data conversion hallucinations from LLM conversions?\n",
        "\n",
        "When using LLMs like 383 gpt for data conversion, they might \"hallucinate\" especially when it comes to larger data sets. In mine particularly, since there were only 15 rows to be marked down it worked and it was pretty straightforward and simple. In fact I think it is worthwhile for quicker conversions and formatting for small datasets, but not for larger or highly precise conversions. To mitigate these hallucinations, chunking the data is a potential strategy. Converting the data into smaller manageable chunks can reduce risk of hallucination. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4.2 GPT Pandas Prompting\n",
        "\n",
        "In this section, you will prompt 383GPT to write pandas code manipulations for you.\n",
        "\n",
        "After working with this data for awhile, we realized we're starting to forget the meanings of the abbreviated column names. Let's ask 383GPT to fix this for us. First, navigate to the [UCI dataset overview](https://archive.ics.uci.edu/dataset/336/chronic+kidney+disease) and copy the abbrevation to name mapping. Then, go to 383GPT and instruct the LLM to provide you with a pandas script to apply this renaming to all the columns of your dataset. Paste that code below and make any adjustments necessary to run it in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to rename all the columns in the dataset\n",
        "column_mapping = {\n",
        "    'age': 'Age',\n",
        "    'bp': 'Blood Pressure',\n",
        "    'bgr': 'Blood Glucose Random',\n",
        "    'bu': 'Blood Urea',\n",
        "    'sc': 'Serum Creatinine',\n",
        "    'sod': 'Sodium',\n",
        "    'pot': 'Potassium',\n",
        "    'hemo': 'Hemoglobin',\n",
        "    'pcv': 'Packed Cell Volume',\n",
        "    'wbcc': 'White Blood Cell Count',\n",
        "    'rbcc': 'Red Blood Cell Count',\n",
        "    'rbc_abnormal': 'Red Blood Cell Abnormal',\n",
        "    'rbc_normal': 'Red Blood Cell Normal',\n",
        "    'pc_abnormal': 'Pus Cell Abnormal',\n",
        "    'pc_normal': 'Pus Cell Normal',\n",
        "    'pcc_notpresent': 'Pus Cell Clumps Not Present',\n",
        "    'pcc_present': 'Pus Cell Clumps Present',\n",
        "    'ba_notpresent': 'Bacteria Not Present',\n",
        "    'ba_present': 'Bacteria Present',\n",
        "    'htn_no': 'Hypertension No',\n",
        "    'htn_yes': 'Hypertension Yes',\n",
        "    'dm_no': 'Diabetes Mellitus No',\n",
        "    'dm_yes': 'Diabetes Mellitus Yes',\n",
        "    'cad_no': 'Coronary Artery Disease No',\n",
        "    'cad_yes': 'Coronary Artery Disease Yes',\n",
        "    'appet_good': 'Appetite Good',\n",
        "    'appet_poor': 'Appetite Poor',\n",
        "    'pe_no': 'Pedal Edema No',\n",
        "    'pe_yes': 'Pedal Edema Yes',\n",
        "    'ane_no': 'Anemia No',\n",
        "    'ane_yes': 'Anemia Yes',\n",
        "    'Target_ckd': 'Chronic Kidney Disease',\n",
        "    'Target_notckd': 'No Chronic Kidney Disease'\n",
        "}\n",
        "\n",
        "new_normalized_sorted_df.rename(columns=column_mapping, inplace=True)\n",
        "\n",
        "new_normalized_sorted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4.3 Augmenting our skills with prompting\n",
        "\n",
        "In addition, we can also use 383GPT to convert our data manipulation operations between different data manipulation languages and libraries. For example let's prompt 383GPT to convert the following SQL query to a pandas query.\n",
        "\n",
        "**SQL Query**\n",
        "```sql\n",
        "SELECT Target, COUNT(*) AS count\n",
        "FROM your_table_name\n",
        "GROUP BY Target;\n",
        "```\n",
        "\n",
        "Prompt 383GPT to convert this to a pandas query. Run this query below, then describe what it does. (If you're not familiar with SQL that is okay you need to only comment on the final resulting output.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converted SQL to Pandas code\n",
        "target_count = new_normalized_sorted_df.groupby('Chronic Kidney Disease').size().reset_index(name='count')\n",
        "target_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Describe what the above code does here*\n",
        "\n",
        "The .groupby('Chronic Kidney Disease') function groups the dataset by the target column (Target_ckd). Then, the .size() counts the number of rows in each group. Lastly, the .reset_index(name='count') creates a new dataframe where the counts are shown in a column. Finally, the output shows us how many rows in the dataset correspond to patients with the disease (CKD) and how many don't have the disease. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
